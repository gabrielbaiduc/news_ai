import json
import logging
import random
import asyncio
from collections import defaultdict
from collections import Counter
from urllib.parse import urljoin

from bs4 import BeautifulSoup
import aiohttp
from dateutil import parser
from dateutil import tz

from utils.helpers import isoutdated
from data_manager.manager import DataManager
from config.settings import selectors, user_agents

# Module to scrape articles. It implements 3 classes, one to handle the network
# operations and two to handle the scraping. For more about the scraping logic
# read 'config/settings.py'.

# ToDo:
# 1) Merge 'ScrapeLink' and 'ScrapeContents' into 'Scraper'
# 2) Move parsing into 'Scraper'
# 3) Move FetchHTML into 'utils/http_requests.py' and combine with 'PostJSON'
#   from 'summarising/summarise.py'

logger = logging.getLogger(__name__)


class FetchHTML:
    """
    Class responsible for making 'GET' requests to news-site URLs. It compiles
    a list of tasks from lists of tuples containing the URL, the source of the 
    URL and the category it belongs to. Read more in 'config/settings.py'.
    Responses are repackaged into tuples containing the same meta-information
    + the HTML content. Exceptions are re-raised up the call stack and stored 
    separately. Note: exceptions are not retried but stored in 'exceptions'.

    Attributes:
        rate_limit (int): the rate-limit of requests
        user_agents (list): list of user-agents
        exceptions (list): list of exceptions raised
    """
    def __init__(self, rate_limit=50):
        """
        Initialise rate-limit (defaul is 50), list of user agents and empty
        exceptions list.

        Params:
            rate_limit (int, optional): the rate limit
        """
        self.rate_limit = rate_limit
        self.user_agents = user_agents
        self.exceptions = []


    async def fetch(self, requests):
        """
        Main method to fethc HTMLs. It takes a list of requests as argument each
        request a 3-tuple; (url, source, category). 
        Creates a session with rate-limiting then creates a list of tasks
        by iterating over the list of requests and passing each request and the
        session object to the utility method `_process_request`. The tasks
        are executed concurrently. Any exceptions raised during the request are
        re-raised back here where 'return_exceptions=True' handles them by
        adding an Exception object to the results. The successful results are
        separated from the exceptions and returned as a list of 4-tuples 
        (url, source, categor, html).

        Params:
            requests (list): list of 3-tuples (url, src, cat)
        Returns:
            results (list): list of 4-tuples (url, src, cat, html)
        """

        # Initialising connector 
        connector = aiohttp.TCPConnector(limit=self.rate_limit)

        # Opening network session 
        async with aiohttp.ClientSession(connector=connector) as session:

            # Creating list of tasks
            tasks = [self._process_request(request, session) 
                        for request in requests]

            # Executing list of tasks 
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Sorting successful requests from exceptions
            results = self._sort_results(results)

            # Logging results
            logger.info(f"Fetched HTML of {len(results)} URLs from "
                f"{len(tasks)} concurrent tasks with "
                f"{len(self.exceptions)} exceptions"
                )

            return results


    def parse(self, results):
        """
        Method that parses HTML content using BS4. It takes a list of 4-tuples
        as argument (usually the list generated by 'fetch') and parses each 
        HTML, returning the same list with the parsed HTML in place of the raw
        HTML.

        Params: 
            results (list): list of 4-tuples (url, src, cat, html)
        Returns:
            parsed (list): list of 4-tuples (url, src, cat, parsed_html)
        """
        # Initialising empty container
        parsed = []

        # Iterating over 'results' and unpacking tuples
        for url, src, cat, html in results:
            # Parsing html
            soup = BeautifulSoup(html, "html.parser")
            # Repacking tuple with parsed html and adding to container
            parsed.append((url, src, cat, soup))
            logger.debug(f"Parsed HTML of {url}")

        # Logging results
        logger.info(f"Parsed HTML of {len(parsed)} URLs.")

        return parsed


    def _sort_results(self, results):
        """
        Takes the list of results from `fetch` and separates the successful 
        requests from the exceptions. Returns a list of the successful 
        requests and stores the exceptions in the 'self.exceptions' attribute.

        Params:
            results (list): contains 4-tuples and possible Exception objects
        Returns:
            success (list): list of 4-tuples (url, src, cat, html)
        """
        # Initialising empty container
        success = []

        # Iterating over 'results'
        for result in results:
            # Checking if Exception object and adding to 'self.exceptions'
            if isinstance(result, Exception):
                self.exceptions.append(result)
            # Adding to container otherwise
            else:
                success.append(result)

        return success


    async def _process_request(self, request, session):
        """
        Async utility function that unpacks the 3-tuple and calls for a request
        on the URL then repacks the 3-tuple and adds the HTML contents.
        When an exception is raised in '_request', the exception is passed up to
        `fetch` (the return statement is skipped)

        Params:
            request (tup): 3-tuple (url, src, cat)
            session (obj): session object
        Returns:
            _ (tuple): 4-tuple like (url, src, cat, html)
            """
        # Unpacking tuple
        url, src, cat = request

        # Making request
        html = await self._request(url, session)

        # Repacking and returning tuple with HTML
        return (url, src, cat, html)


    async def _request(self, url, session):
        """
        Makes the individual requests. It raises an exception for 4xx and 5xx
        status codes or waits for the HTML contents. 

        Params:
            url (str): the url
            session (obj): the session object
        Returns:
            _ (str): html content or re-raised exception
        """
        # Initialising header with random user-agent
        headers = {'User-Agent': self._get_random_user_agent()}

        try:
            # Attempting to connect
            async with session.get(url, headers=headers) as response:
                # Raising client and server errors
                response.raise_for_status()
                logger.debug(f"Fetched HTML of {url}.")
                # Waiting for response contents
                return await response.text()

        # Re-raising exception
        except aiohttp.ClientResponseError as error:
            # Logging exception
            logger.error(f"{error.status} {error.message} {url}")
            raise


    def _get_random_user_agent(self):
        """
        Selects a random user agent from a list of user agents list.

        Returns:
            _ (str): A random user agent string.
        """
        return random.choice(self.user_agents)

    

class ScrapeLinks:
    """
    Creates a scraper object. The object is responsible for scraping article
    links from section URLs - for more information about sections, what they do,
    and how they define the scraping process, read 'config/settings.py'. The 
    object consists of the main `scrape` method and various utility methods that
    perform subtasks like retrieving target HTML elements or performing checks.

    Attributes:
        manager (class obj.): 'DataManager' class object used to load and save
        data files from disc. The data files are used for checking.
        articles (list): list of article dictionaries
        discarded (list): list of previously discarded URLs
        link_selector (dict): a dict. of source: selector pairs
        link_prefix (dict): a dict of source: prefix pairs
    """
    def __init__(self):
        """ Initiaises class specific attributes. """
        self.manager = DataManager()
        self.articles = self.manager.load()
        self.discarded = self.manager.load("discarded")
        self.link_selector = selectors["link_selector"]
        self.link_prefix = selectors["link_prefix"]


    def scrape(self, sections):
        """
        Main scraping method. Takes as argument the list of 4-tuples containing
        the section URLs and their HTMLs with associated sources and categories; 
        (url, src, cat, html). It creates an empty list and iterates 
        over the list of 4-tuples and for each element it:
        1) attempts to extract the link elements using source specific, static 
            selectors. If no elements are returned, it logs an error and skips 
            this URL. Bad selectors can cause this.
        2) extracts the 'href' attribute from the elements
        3) combines the 'href' with a source specific prefix to form links
        4) checks the resulting links against known links.
            If all the links from this URL are known to be processed the URL
            is skipped
        5) we package each link with the source and category in a 3-tuple 
            and add the 3-tuple to the container list from above.
        Once all URLs are processed like this, all the links in the populated 
        container are checked, for identical links, the categories are compared
        if the categories don't match, the new category as added to the existing
        link.

        Params:
            parsed (list): list of 4-tuples like (url, src, cat, html)
        Returns:
            checked_links (list): list of 3-tuples like (url, src, cat)
        """
        # Initialising empty container
        new_links = []
        logger.debug(f"Scraping {len(sections)} URLs")

        # Iterating over list of 4-tuples
        for url, src, cat, soup in sections:

            # scraping HTML elements
            elements = self._getelements(soup, src, url)
            if not elements:
                continue

            # Extracing 'href' attributes
            hrefs = self._gethrefs(elements, src)

            # Constructing links
            links = self._construct_links(hrefs, src)

            # Checking against known links
            links = self._check_processed(links)
            if not links:
                logger.debug(f"No new links from {url} check ScrapeLinks")
                continue

            # Packing links in 3-tuples
            links = [(link, src, cat) for link in links]

            # Extend container list with list of 3-tuples.
            new_links.extend(links)
            logger.debug(f"Scraped {len(links)} new links from {url}")

        # Check for multi-category links
        checked_links = self._check_category(new_links)

        return checked_links


    def _check_category(self, links):
        """ 
        Takes a list of 3-tuples. It iterates over the list and keeps track of 
        each link in a dict. Each new link is compared to the dictionary,
        if an identical link is found, the link's categories are compared, if 
        they are different the category of the second link is added to the 
        category of the first link and the second link is discarded.
        A list of 3-tuples is returned in the form (url, src, [cats])

        Params:
            links (list): list of 3-tuples like (url, src, cat)
        Returns:
            _ (list): list of 3-tuples like (url, src, [cat])
        """
        # Initialising temporary dict
        temp = {}

        # Iterating over 3-tuples
        for link, src, cat in links:
            # Checking for identical links with different categories
            if link in temp and cat not in temp[link]["cats"]:
                # Extending first link's category list
                temp[link]["cats"].append(cat)
                logger.debug(f"Added category {link}")
            # Checking if a link is duplicated
            elif link in temp and cat in temp[link]["cats"]:
                logger.debug(f"Duplicate {link}")
                continue
            # Addign new links to temporary dict.
            elif link not in temp:
                temp[link] = {"src": src, "cats": [cat]}

        return [
        (link, info['src'], info['cats']) for link, info in temp.items()
        ]


    def _getelements(self, soup, src, url):
        """
        Using a source specific CSS selector, it selects the the HTML elements
        that contain potential article links. Returns a list of elements or None

        Params:
            soup (BS4 obj.): parsed section HTML
            src (str): the source, used to index the source specific selector
            url (str): the section URL, used for logging selector errors
        Returns:
            _ (list): list of HTML elements
        """
        # Indexing selector
        selector = self.link_selector[src]

        # Extracing list of elements
        elements = soup.select(selector)

        # Validating list of elements
        if elements:
            return elements

        # Logging erro if failed validation
        logger.error(f"Link Selector is broken {src} {selector} {url}")


    def _gethrefs(self, elements, src):
        """
        Retrieves the 'href' attributes from the list of HTML elements. 
        For NYT links it performs a check to filter links that are not articles.
        Returns a list of 'hrefs'.

        Params:
            elements (BS4 obj.): HTML elements
            src (str): the source, used to check if elements are from NYT
        Returns:
            hrfes (list): list of hrefs.
        """
        # Retrieving 'hrefs'
        hrefs = [
            element["href"] for element in elements if element.get("href")
        ]

        # Checking for non-articles when source is NYTimes
        if src == "NYTimes":
            hrefs = self._check_nyt_notarticle(hrefs)

        return hrefs


    def _construct_links(self, hrefs, src):
        """
        Indexes source specific prefix and combines it with the hrefs 
        from the href list.

        Params:
            hrefs (list): list of hrefs
            src (str): the source, used for indexing the source specific prefix
        Returns 
            _ (list): list of combined links
        """
        # Indexing prefix
        prefix = self.link_prefix[src]

        # Combining and returning list of links
        return [urljoin(prefix, href) for href in hrefs]


    def _check_nyt_notarticle(self, hrefs):
        """
        Checks `hrefs` for particular terms to filter out non-articles when
        the source is the NYTimes.

        Params:
            hrefs (list): list of 'hrefs'
        Returns:
            _ (list): list of 'hrefs' with non-articles filtered
        """
        # Returning 'hrefs' that don't match the pattern
        return [
            href for href in hrefs
            if not href.startswith("/interactive")
            and not href.startswith("/video")
        ]


    def _check_processed(self, links):
        """
        Takes a list of links and compares them to links that are known to have
        been processed before (either scraped or discarded for various reasons).
        Returns a list of filtered links.

        Params:
            links (list): list of links
        Returns:
            links (list): list of links
        """
        # Validates existing data
        if self.articles and self.discarded:

            # scraping links from scraped articles
            processed = [article["url"] for article in self.articles]

            # Filtering links
            filtered = [link for link in links if 
            link not in processed and link not in self.discarded
            ]
            logger.debug(f"Filtered {len(links)-len(filtered)} links from "
                f"{len(links)}."
            )

            return filtered

        # Returning links as they are when no prior data exists, usually on 
        # first run.
        else:
            return links


    def count_newlinks(self, new_links):
        """ 
        Method counts new articles from each source. It uses a Counter object
        from 'collections' to count articles with the same 'src'. Since
        'new_links' is a tuple, we select the 2nd element, known to be the 
        source.

        Params:
            new_links (list): list of links
        """
        # Initialising Counter object and counting articles
        counter = Counter([a[1] for a in new_links])

        # Logging results
        logger.info(f"New links:")
        [logger.info(f"{v, c}") for v, c in counter.items()]


class ScrapeContents:
    """
    Class responsible for scraping article contents from article links. 
    Similarly to `ScrapeLinks`, the main method is `scrape` which utilises a 
    collection of utility methods to extract and validate article contents. 
    `scrape` returns a list of dictionaries, where each dictionary is an 
    article. 

    Attributes:
        manager (class obj): DataManager object, used for managing discarded 
        links
        discarded (list): lost of links loaded form disc
        header_selector (dict): dict of source: selector pairs
        text_selector (dict): dict of source: selector pairs
    """
    def __init__(self):
        """ Init method initialises class attributes. """
        self.manager = DataManager()
        self.discarded = self.manager.load("discarded")
        self.header_selector = selectors["header_selector"]
        self.text_selector = selectors["text_selector"]


    def scrape(self, parsed):
        """
        Main scraping method. Takes as argement a list of 4-tuples like 
        (url, src, cat, html) and returns a list of article dictionaries.
        Creates an empty container list then it
        iterates over the list of 4-tuples, for each elements it unpacks the
        tuple and:
        1) extracts and validates the header, if None; discard and skip
        2) retrieves dates, headline and description from header, if None; 
            discard and skip
        3) validates date, if article older than 24hrs, discard and skip
        4) extract body, if None, discard and skip
        5) build article dictionary and add to container list
        Once finished, the list of discarded URLs is saved and the container
        list is returned.
        """
        logger.debug(f"Scraping {len(parsed)} URLs")
        # Initialising empty container
        articles = []

        # Iterating over list of 4-tuples
        for url, src, cat, soup in parsed:

            # Scraping header 
            header = self._getheader(soup, src, url)
            # Validating header
            if not header or self._check_notarticle(header, url):
                self._discard(url)
                continue

            # Retrieving header contents
            header_contents = self._getheader_contents(header, url)
            # Validating header contents
            if not header_contents:
                self._discard(url)
                continue
            # Unpacking header contents
            pub, mod, hline, desc = header_contents

            # Checking date
            if isoutdated(pub):
                self._discard(url)
                logger.debug(f"Outdated {url}")
                continue

            # Scraping body
            body = self._getbody(soup, src, url)
            # Validating body
            if not body:
                self._discard(url)
                continue

            # Building article
            article = self._buildarticle(
                url, src, cat, pub, mod, hline, desc, body
                )
            # Adding article to container
            articles.append(article)
            logger.debug(f"Scraped article from {url}")

        # Saving discarded URLs
        self.manager.save(self.discarded, "discarded")

        # Logging results
        logger.info(f"Scraped {len(articles)} new articles from {len(parsed)} "
            f"links")

        return articles
    

    def _getheader(self, soup, src, url):
        """
        Method responsible for scraping and turning the header element's 
        contents into a dictionary. A source specific, static selector is used 
        to scrape the element. The element is validated (check if None) 
        and the text contents are extracted and turned into a dictionary. 
        Except for decoding errors. If validation fails or decoder error occurs
        nothing ir returned, otherwise the dictionary is returned.

        Params:
            soup (BS4 obj.): parsed html
            src (str): the source
            url (str): the url
        Returns:
            header (dict): or None
        """
        # Indexing selector
        selector = self.header_selector[src]

        # Scraping element
        element = soup.select_one(selector)

        # Validating element
        if not element:
            # Logging error, nothing is returned
            logger.error(f"Header selector is broken: {src} {selector} {url}")    
            return None

        # Extracting text from element
        string = element.get_text()

        # Attempting to conert to dictionary
        try:
            header = json.loads(string)

        # Logging error, nothing is returned
        except JSONDecodeError as error:
            logger.error(f"Header is not in JSON format: {error}")
            return None

        # Handling site-specific formatting differences
        if isinstance(header, list):
            header = header[0]
        logger.debug(f"Scraped header of {url}")

        return header


    def _getheader_contents(self, header, url):
        """
        Method responsible for retrieving dates, the headline and the 
        description from the header dictionary. Dates are parsed using
        the utility method '_parse_datetime'. 'KeyError' exceptions are logged
        and nothing is returned, otherwise a tuple is returned with the data 
        (published, modified, headline, description)

        Params:
            header (dict): the scraped header
            url (str): the url
        Returns:
            _ (tuple): (published, modified, headline, description) or None
        """
        # Attempting to retrieve data from header
        try:
            pub = self._parse_datetime(header["datePublished"])
            mod = self._parse_datetime(header["dateModified"])
            hline = header["headline"]
            desc = header["description"]
            logger.debug(f"Scraped header contents of {url}")
            return pub, mod, hline, desc

        # Logging exception, returning nothing
        except KeyError as error:
            logger.error(
                f"Header key {error} is broken or not an article: {url}"
                )
            return None
    

    def _getbody(self, soup, src, url):
        """
        Method responsible for scraping the article body. A source specific,
        static selector is used. The element is validated (check if None). The
        paragraphs are retrieved from the elements, ensuring no empty 
        paragraphs are included. The list of paragraphs are joined and ones 
        shorter than 100 words are logged as warnings (likely not articles). 
        If validation fails or body too short; nothing is returned, otherwise
        string is returned.

        Params:
            soup (obj):  parsed html
            src (str): the source
            url (str): the url
        Returns:
            body (str): or None
        """
        # Indexing selector
        selector = self.text_selector[src]

        # Scraping element
        elements = soup.select(selector)

        # Validating element
        if not elements:
            logger.error(f"Text selector is broken: {url} {selector}")
            return None

        # Retrieving paragraphs from elements
        paragraphs = [element.text.strip() for element in elements]
        logger.debug(f"Scraped {len(paragraphs)} paragraphs ... ")

        # Filtering empty paragraphs 
        paragraphs = [p for p in paragraphs if len(p.split()) > 0]
        logger.debug(f"... of which {len(paragraphs)} is non-empty.")

        # Joining body
        body = " ".join(paragraphs)

        # Validating body lenght
        if len(body.split()) < 100:
            logger.warning(f"Body too short or not an article: {url}")
            return None
        logger.debug(f"Scraped body of {url}.")

        return body
        

    def _parse_datetime(self, string):
        """
        Turns the published and modified dates to datetime objects. Standardises
        datetime objects to UTC timezone.

        Params:
            string (str): the date in string format
        Returns:
            standardised_datetime (obj): standardised datetime object
        """
        # Parsing datetime
        parsed_datetime = parser.isoparse(string)

        # Standardising datetime
        standardised_datetime = parsed_datetime.astimezone(tz.tzutc())

        return standardised_datetime


    def _check_notarticle(self, header, url):
        """
        Checks if the article header indicates that the content is not a
        standard article, such as a video.
        """
        if header.get("@type") == "VideoObject":
            logger.warning(f"Not an article {url}")
            return True
        return False


    def _buildarticle(self, url, src, cat, pub, mod, hline, desc, body):
        """
        Constructs a dictionary representing the article with all relevant
        details.

        Params: all the scraped data + the metadata 
        Returns: (dict) the article
        """
        return {
            "bodycount": len(body.split()),
            "source": src,
            "category": cat,
            "headline": hline,
            "published": pub,
            "modified": mod,
            "url": url,
            "description": desc,
            "body": body,
        }


    def _discard(self, url):
        """
        Stores URL in 'discarded'.

        Params:
            url (str):
        """
        self.discarded.append(url)
        logger.info(f"Discarded {url}")